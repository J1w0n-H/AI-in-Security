"""
Vulnerability ranking module for IRIS.

This module provides environment-weighted ranking of vulnerabilities
based on their actual exploitability in the specific environment.
"""

import json
from typing import Dict, List, Any, Optional, Tuple
from pathlib import Path


class VulnerabilityRanker:
    """Ranks vulnerabilities based on environment-weighted risk scores."""
    
    def __init__(self, logger=None):
        self.logger = logger
    
    def rank_vulnerabilities(self, 
                           vulnerabilities: List[Dict[str, Any]],
                           environment_context: Dict[str, Any]) -> List[Dict[str, Any]]:
        """
        Rank vulnerabilities based on environment-weighted risk scores.
        
        Args:
            vulnerabilities: List of vulnerability analysis results
            environment_context: Environment context for weighting
            
        Returns:
            Ranked list of vulnerabilities with scores
        """
        # 추가된 부분 from here
        ranked_vulnerabilities = []
        
        for vuln in vulnerabilities:
            # Calculate comprehensive risk score
            risk_score = self._calculate_comprehensive_risk_score(vuln, environment_context)
            
            # Add ranking information
            vuln_with_ranking = vuln.copy()
            vuln_with_ranking.update({
                "comprehensive_risk_score": risk_score,
                "ranking_factors": self._get_ranking_factors(vuln, environment_context),
                "priority_level": self._determine_priority_level(risk_score)
            })
            
            ranked_vulnerabilities.append(vuln_with_ranking)
        
        # Sort by comprehensive risk score (descending)
        ranked_vulnerabilities.sort(key=lambda x: x["comprehensive_risk_score"], reverse=True)
        
        # Add rank position
        for i, vuln in enumerate(ranked_vulnerabilities):
            vuln["rank"] = i + 1
        
        return ranked_vulnerabilities
        # 추가된 부분 to here
    
    def _calculate_comprehensive_risk_score(self, 
                                          vuln: Dict[str, Any],
                                          environment_context: Dict[str, Any]) -> float:
        """Calculate comprehensive risk score for a vulnerability."""
        # 추가된 부분 from here
        # Base risk score from environment analysis
        base_score = vuln.get("environment_score", 0)
        
        # KB risk score
        kb_score = vuln.get("kb_risk_score", 0)
        
        # LLM confidence score
        llm_confidence = self._convert_confidence_to_score(vuln.get("confidence", "MEDIUM"))
        
        # Dynamic verification result
        dyn_verification = vuln.get("dynamic_verification")
        dyn_score = self._calculate_dynamic_score(dyn_verification)
        
        # Environment-specific adjustments
        env_adjustment = self._calculate_environment_adjustment(vuln, environment_context)
        
        # Sanitizer effectiveness adjustment
        sanitizer_adjustment = self._calculate_sanitizer_adjustment(vuln)
        
        # Calculate final score
        final_score = (
            base_score * 0.3 +
            kb_score * 0.25 +
            llm_confidence * 0.2 +
            dyn_score * 0.15 +
            env_adjustment * 0.05 +
            sanitizer_adjustment * 0.05
        )
        
        return max(0, min(10, final_score))  # Clamp between 0 and 10
        # 추가된 부분 to here
    
    def _convert_confidence_to_score(self, confidence: str) -> float:
        """Convert LLM confidence to numeric score."""
        # 추가된 부분 from here
        confidence_map = {
            "HIGH": 8.0,
            "MEDIUM": 5.0,
            "LOW": 2.0
        }
        return confidence_map.get(confidence, 5.0)
        # 추가된 부분 to here
    
    def _calculate_dynamic_score(self, dyn_verification: Optional[Dict[str, Any]]) -> float:
        """Calculate score based on dynamic verification results."""
        # 추가된 부분 from here
        if not dyn_verification:
            return 5.0  # Neutral score if no dynamic verification
        
        if dyn_verification.get("verified", False):
            if dyn_verification.get("test_result") == "PASS":
                return 9.0  # High score for confirmed vulnerability
            elif dyn_verification.get("test_result") == "FAIL":
                return 1.0  # Low score for safe behavior
            else:
                return 5.0  # Neutral for other results
        else:
            return 3.0  # Lower score for unverified
        # 추가된 부분 to here
    
    def _calculate_environment_adjustment(self, 
                                        vuln: Dict[str, Any],
                                        environment_context: Dict[str, Any]) -> float:
        """Calculate environment-specific adjustment."""
        # 추가된 부분 from here
        adjustment = 0.0
        
        # OS-specific adjustments
        os_type = environment_context.get("os", "unknown")
        if os_type == "windows":
            # Windows-specific vulnerabilities might be more critical
            if "command.exec" in vuln.get("kb_rule_ids", []):
                adjustment += 1.0
            if "path.access" in vuln.get("kb_rule_ids", []):
                adjustment += 0.5
        elif os_type == "linux":
            # Linux-specific adjustments
            if "command.exec" in vuln.get("kb_rule_ids", []):
                adjustment += 0.5
        
        # Container-specific adjustments
        if environment_context.get("containerized", False):
            # Container vulnerabilities might be more critical
            adjustment += 0.5
        
        # Security policy adjustments
        policies = environment_context.get("policies", {})
        if policies.get("selinux") == "enabled":
            # SELinux enabled might reduce some risks
            adjustment -= 0.3
        if policies.get("apparmor") == "enabled":
            # AppArmor enabled might reduce some risks
            adjustment -= 0.3
        
        return adjustment
        # 추가된 부분 to here
    
    def _calculate_sanitizer_adjustment(self, vuln: Dict[str, Any]) -> float:
        """Calculate adjustment based on sanitizer effectiveness."""
        # 추가된 부분 from here
        adjustment = 0.0
        
        sanitizer_effectiveness = vuln.get("sanitizer_effectiveness", {})
        
        for sanitizer, effectiveness in sanitizer_effectiveness.items():
            if effectiveness.get("effective", True):
                adjustment -= 1.0  # Reduce risk if sanitizer is effective
            else:
                adjustment += 0.5  # Increase risk if sanitizer is ineffective
        
        return adjustment
        # 추가된 부분 to here
    
    def _get_ranking_factors(self, 
                           vuln: Dict[str, Any],
                           environment_context: Dict[str, Any]) -> List[str]:
        """Get factors that influenced the ranking."""
        # 추가된 부분 from here
        factors = []
        
        # High risk factors
        if vuln.get("environment_score", 0) >= 7:
            factors.append("High environment risk score")
        
        if vuln.get("kb_risk_score", 0) >= 3:
            factors.append("High KB risk score")
        
        if vuln.get("confidence") == "HIGH":
            factors.append("High LLM confidence")
        
        # Dynamic verification factors
        dyn_verification = vuln.get("dynamic_verification")
        if dyn_verification and dyn_verification.get("verified"):
            if dyn_verification.get("test_result") == "PASS":
                factors.append("Dynamic verification confirmed vulnerability")
            elif dyn_verification.get("test_result") == "FAIL":
                factors.append("Dynamic verification showed safe behavior")
        
        # Environment factors
        env_factors = vuln.get("environment_factors", [])
        factors.extend(env_factors)
        
        # Sanitizer factors
        sanitizer_effectiveness = vuln.get("sanitizer_effectiveness", {})
        for sanitizer, effectiveness in sanitizer_effectiveness.items():
            if not effectiveness.get("effective", True):
                factors.append(f"Ineffective sanitizer: {sanitizer}")
        
        return factors
        # 추가된 부분 to here
    
    def _determine_priority_level(self, risk_score: float) -> str:
        """Determine priority level based on risk score."""
        # 추가된 부분 from here
        if risk_score >= 8.0:
            return "CRITICAL"
        elif risk_score >= 6.0:
            return "HIGH"
        elif risk_score >= 4.0:
            return "MEDIUM"
        elif risk_score >= 2.0:
            return "LOW"
        else:
            return "INFO"
        # 추가된 부분 to here
    
    def generate_ranking_report(self, 
                              ranked_vulnerabilities: List[Dict[str, Any]],
                              top_n: int = 10) -> Dict[str, Any]:
        """Generate a ranking report for the vulnerabilities."""
        # 추가된 부분 from here
        total_vulns = len(ranked_vulnerabilities)
        top_vulns = ranked_vulnerabilities[:top_n]
        
        # Calculate statistics
        priority_counts = {}
        for vuln in ranked_vulnerabilities:
            priority = vuln.get("priority_level", "INFO")
            priority_counts[priority] = priority_counts.get(priority, 0) + 1
        
        # Calculate average scores
        avg_risk_score = sum(vuln.get("comprehensive_risk_score", 0) for vuln in ranked_vulnerabilities) / total_vulns if total_vulns > 0 else 0
        avg_env_score = sum(vuln.get("environment_score", 0) for vuln in ranked_vulnerabilities) / total_vulns if total_vulns > 0 else 0
        
        return {
            "summary": {
                "total_vulnerabilities": total_vulns,
                "top_n_shown": top_n,
                "average_risk_score": round(avg_risk_score, 2),
                "average_environment_score": round(avg_env_score, 2),
                "priority_distribution": priority_counts
            },
            "top_vulnerabilities": top_vulns,
            "ranking_criteria": {
                "environment_score_weight": 0.3,
                "kb_risk_score_weight": 0.25,
                "llm_confidence_weight": 0.2,
                "dynamic_verification_weight": 0.15,
                "environment_adjustment_weight": 0.05,
                "sanitizer_adjustment_weight": 0.05
            }
        }
        # 추가된 부분 to here
